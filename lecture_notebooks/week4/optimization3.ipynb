{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization 3: Constraints and Linear Programming\n",
    "\n",
    "Florian Oswald\n",
    "Sciences Po, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Constraints\n",
    "\n",
    "Recall our core optimization problem:\n",
    "\n",
    "$$ \n",
    "\\min_{x\\in\\mathbb{R}^n} f(x)  \\text{ s.t. } x \\in \\mathcal{X}\n",
    "$$\n",
    "\n",
    "* Up to now, the feasible set was $\\mathcal{X} \\in \\mathbb{R}^n$. \n",
    "* In **constrained problems** $\\mathcal{X}$ is a subset thereof.\n",
    "* We already encountered *box constraints*, e.g. $x \\in [a,b]$.\n",
    "* Sometimes the contrained solution coincides with the unconstrained one, sometimes it does not.\n",
    "* There are *equality constraints* and *inequality constraints*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lagrange Multipliers\n",
    "\n",
    "* Used to optimize a function subject to equality constraints.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x & f(x) \\\\\n",
    "\\text{subject to } & h(x) = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where both $f$ and $h$ have continuous partial derivatives.\n",
    "\n",
    "* We look for contour lines of $f$ that are aligned to contours of $h(x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, we want to find the best $x$ s.t. $h(x) = 0$ and we have\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\lambda \\nabla h(x)\n",
    "$$\n",
    "\n",
    "for some *Lagrange Mutliplier* $\\lambda$\n",
    "* Notice that we need the scalar $\\lambda$ because the magnitudes of the gradients may be different.\n",
    "* We therefore form the the **Lagrangian**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x,\\lambda) = f(x) - \\lambda h(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Suppose we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x & -\\exp\\left( -\\left( x_1 x_2 - \\frac{3}{2} \\right)^2 - \\left(x_2 - \\frac{3}{2}\\right)^2 \\right) \\\\\n",
    "\\text{subject to } & x_1 - x_2^2 = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We form the Lagrangiagn:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x_1,x_2,\\lambda) = -\\exp\\left( -\\left( x_1 x_2 - \\frac{3}{2} \\right)^2 - \\left(x_2 - \\frac{3}{2}\\right)^2 \\right) - \\lambda(x_1 - x_2^2)\n",
    "$$\n",
    "\n",
    "Then we compute the gradient wrt to $x_1,x_2,\\lambda$, set to zero and solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "\n",
    "x=0:0.01:3.5\n",
    "f0(x1,x2) = -exp.(-(x1.*x2 - 3/2).^2 - (x2-3/2).^2)\n",
    "c(z) = sqrt(z)\n",
    "\n",
    "p1 = surface(x,x,(x,y)->f0(x,y),xlab = L\"x_1\", ylab = L\"x_2\")\n",
    "scatter3d!(p1,[1.358],[1.165],[f0(1.358,1.165)],markercolor=:red,leg=false)\n",
    "p2 = contour(x,x,(x,y)->f0(x,y),lw=1.5,levels=[collect(0:-0.1:-0.85)...,-0.887,-0.95,-1],xlab = L\"x_1\", ylab = L\"x_2\")\n",
    "plot!(p2,c,0.01,3.5,label=\"\",lw=2,color=:black,fill=(0,0.5,:blue))\n",
    "scatter!(p2,[1.358],[1.165],markersize=5,markercolor=:red,label=\"Constr. Optimum\")\n",
    "plot(p1,p2,size=(900,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If we had multiple constraints ($l$), we'd just add them up to get\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x},\\mathbf{\\lambda}) = f(\\mathbf{x}) - \\sum_{i=1}^l \\lambda_i h_i(\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inequality Constraints\n",
    "\n",
    "Suppose now we had\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x} & f(\\mathbf{x}) \\\\\n",
    "\\text{subject to } & g(\\mathbf{x}) \\leq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which, if the solution lies *on* the constraint *boundary*, means that\n",
    "\n",
    "$$\n",
    "\\nabla f - \\mu \\nabla g = 0\n",
    "$$\n",
    "\n",
    "for some scalar $\\mu$ - as before. \n",
    "\n",
    "* In this case, we say the **constraint is active**.\n",
    "* In the opposite case, i.e. the solution lies **inside** the contrained region, we way the contraint is **inactive**. \n",
    "* In that case, we are back to an *unconstrained* problem, look for $\\nabla f = 0$, and set $\\mu=0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the blue area shows the FEASIBLE SET\n",
    "contour(x,x,(x,y)->f(x,y),lw=1.5,levels=[collect(0:-0.1:-0.85)...,-0.887,-0.95,-1])\n",
    "plot!(c,0.01,3.5,label=\"\",lw=2,color=:black,fill=(0,0.5,:blue))\n",
    "scatter!([1.358],[1.165],markersize=5,markercolor=:red,label=\"Constr. Optimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the blue area shows the FEASIBLE SET\n",
    "# NOW THE CONSTRAINT IS INACTIVE OR SLACK!\n",
    "c2(x1) = 1+sqrt(x1)\n",
    "contour(x,x,(x,y)->f(x,y),lw=1.5,levels=[collect(0:-0.1:-0.85)...,-0.887,-0.95,-1])\n",
    "plot!(c2,0.01,3.5,label=\"\",lw=2,color=:black,fill=(0,0.5,:blue))\n",
    "scatter!([1],[1.5],markersize=5,markercolor=:red,label=\"Unconstr. Optimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Infinity Step\n",
    "\n",
    "* We could do an **infinite step** to avoid *infeasible points*:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\infty\\text{-step}} &= \\begin{cases}\n",
    "f(\\mathbf{x}) & \\text{if } g(\\mathbf{x}) \\leq 0 \\\\\n",
    "\\infty & \\text{else. } \n",
    "\\end{cases}\\\\\n",
    " &= f(\\mathbf{x}) + \\infty (g(\\mathbf{x} > 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Unfortunately, this is discontinous and non-differentiable, i.e. hard to handle for algorithms.\n",
    "* Instead, we use a *linear penalty* $\\mu g(\\mathbf{x})$ on the objective if the constraint is violated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The penalty provides a lower bound to $\\infty$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x},\\mu) = f(\\mathbf{x}) + \\mu g(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "* We can get back the infinite step by maximizing the penalty:\n",
    "\n",
    "$$\n",
    "f_{\\infty\\text{-step}} = \\max_{\\mu\\geq 0} \\mathcal{L}(\\mathbf{x},\\mu)\n",
    "$$\n",
    "\n",
    "* Every infeasible $\\mathbf{x}$ returns $\\infty$, all others return $f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kuhn-Karush-Tucker (KKT)\n",
    "\n",
    "* Our problem thus becomes\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} \\max_{\\mu\\geq 0} \\mathcal{L}(\\mathbf{x},\\mu)\n",
    "$$\n",
    "\n",
    "* This is called the **primal problem**. Optimizing this requires:\n",
    "\n",
    "\n",
    "1. $g(\\mathbf{x}^*) \\leq 0$. Point is feasible.\n",
    "2. $\\mu \\geq 0$. Penalty goes into the right direction. *Dual feasibility*.\n",
    "3. $\\mu g(\\mathbf{x}^*) = 0$. Feasible point on the boundary has $g(\\mathbf{x}) = 0$, otherwise $g(\\mathbf{x}) < 0$ and $\\mu =0$.\n",
    "4. $\\nabla f(\\mathbf{x}^*) - \\mu \\nabla g(\\mathbf{x}^*) = 0$. With an active constraint, we want parallel contours of objective and constraint. When inactive, our optimum just has $\\nabla f(\\mathbf{x}^*) = 0$, which means $\\mu = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The preceding four conditions are called the Kuhn-Karush-Tucker (KKT) conditions. In the above order, and in general terms, they are:\n",
    "\n",
    "1. Feasibility\n",
    "2. Dual Feasibility\n",
    "3. Complementary Slackness\n",
    "4. Stationarity.\n",
    "\n",
    "The KKT conditions are the FONCs for problems with smooth constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Duality\n",
    "\n",
    "We can combine equality and inequality constraints:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x},\\mathbf{\\lambda},\\mathbf{\\mu}) = f(\\mathbf{x}) + \\sum_{i} \\lambda_i h_i(\\mathbf{x}) + \\sum_j \\mu_j g_j(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where, notice, we reverted the sign of $\\lambda$ since this is unrestricted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The Primal problem is identical to the original problem and just as difficult to solve:\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} \\max_{\\mathbf{\\mu}\\geq 0,\\mathbf{\\lambda}} \\mathcal{L}(\\mathbf{x},\\mathbf{\\mu},\\mathbf{\\lambda})\n",
    "$$\n",
    "\n",
    "* The Dual problem reverses min and max:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{\\mu}\\geq 0,\\mathbf{\\lambda}} \\min_\\mathbf{x}  \\mathcal{L}(\\mathbf{x},\\mathbf{\\mu},\\mathbf{\\lambda})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dual Values\n",
    "\n",
    "* The *max-min-inequality* states that for any function $f(a,b)$\n",
    "\n",
    "$$\n",
    "\\max_\\mathbf{a} \\min_\\mathbf{b} f(\\mathbf{a},\\mathbf{b}) \\leq \\min_\\mathbf{b} \\max_\\mathbf{a} f(\\mathbf{a},\\mathbf{b}) \n",
    "$$\n",
    "\n",
    "* Hence, the solution to the dual is a lower bound to the solution of the primal problem.\n",
    "* The solution to the *dual function*, $\\min_\\mathbf{x}  \\mathcal{L}(\\mathbf{x},\\mathbf{\\mu},\\mathbf{\\lambda})$ is the min of a collection of linear functions, and thus always concave.\n",
    "* It is easy to optimize this.\n",
    "* In general, solving the dual is easy whenever minimizing $\\mathcal{L}$ wrt $x$ is easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Penalty Methods\n",
    "\n",
    "* We can convert the constrained problem back to unconstrained by adding penalty terms for constraint violoations.\n",
    "* A simple method could just count the number of violations:\n",
    "\n",
    "$$\n",
    "p_\\text{count}(\\mathbf{x}) = \\sum_{i} (h_i(\\mathbf{x}) \\neq 0 ) + \\sum_j  (g_j(\\mathbf{x} > 0)\n",
    "$$\n",
    "\n",
    "* and add this to the objective in an *unconstrained* problem with penalty $\\rho > 0$\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} f(\\mathbf{x}) + \\rho p_\\text{count}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "* One can choose the penalty function: for example, a quadratic penaly will produce a smooth objective function\n",
    "* Notice that $\\rho$ needs to become very large sometimes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Augmented Lagrange Method\n",
    "\n",
    "* This is very similar, but specific to equality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interior Point Method\n",
    "\n",
    "* Also called *barrier method*.\n",
    "* These methods make sure that the search point remains always feasible.\n",
    "* As one approaches the constraint boundary, the barrier function goes to infinity. Properties:\n",
    "\n",
    "1. $p_\\text{barrier}(\\mathbf{x})$ is continuous\n",
    "2. $p_\\text{barrier}(\\mathbf{x})$ is non negative\n",
    "3. $p_\\text{barrier}(\\mathbf{x})$ goes to infinitey as one approaches the constraint boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Barriers\n",
    "\n",
    "* Inverse Barrier\n",
    "\n",
    "$$\n",
    "p_\\text{barrier}(\\mathbf{x}) = -\\sum_i \\frac{1}{g_i(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "* Log Barrier\n",
    "\n",
    "$$\n",
    "p_\\text{barrier}(\\mathbf{x}) = -\\sum_i \\begin{cases}\\log(-g_i(\\mathbf{x})) & \\text{if } g_i(\\mathbf{x}) \\geq -1 \\\\\n",
    "0& \\text{else.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* The approach is as before, one transforms the problem to an unconstrained one and increases $\\rho$ until convergence:\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} f(\\mathbf{x}) + \\frac{1}{\\rho} p_\\text{barrier}(\\mathbf{x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "$$ \n",
    "\\min_{x \\in \\mathbb{R}^2} \\sqrt{x_2} \\text{ subject to }\\begin{array}{c} \\\\\n",
    " x_2 \\geq 0 \\\\\n",
    " x_2 \\geq (a_1 x_1 + b_1)^3 \\\\\n",
    "x_2 \\geq (a_2 x_1 + b_2)^3 \n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constrained Optimisation with [`NLopt.jl`](https://github.com/JuliaOpt/NLopt.jl)\n",
    "\n",
    "* We need to specify one function for each objective and constraint.\n",
    "* Both of those functions need to compute the function value (i.e. objective or constraint) *and* it's respective gradient. \n",
    "* `NLopt` expects contraints **always** to be formulated in the format \n",
    "\t$$ g(x) \\leq 0 $$\n",
    "     where $g$ is your constraint function\n",
    "* The constraint function is formulated for each constraint at $x$. it returns a number (the value of the constraint at $x$), and it fills out the gradient vector, which is the partial derivative of the current constraint wrt $x$.\n",
    "* There is also the option to have vector valued constraints, see the documentation.\n",
    "* We set this up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "count = 0 # keep track of # function evaluations\n",
    "\n",
    "function myfunc(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 0\n",
    "        grad[2] = 0.5/sqrt(x[2])\n",
    "    end\n",
    "\n",
    "    global count\n",
    "    count::Int += 1\n",
    "    println(\"f_$count($x)\")\n",
    "\n",
    "    sqrt(x[2])\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function myconstraint(x::Vector, grad::Vector, a, b)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 3a * (a*x[1] + b)^2\n",
    "        grad[2] = -1\n",
    "    end\n",
    "    (a*x[1] + b)^3 - x[2]\n",
    "end\n",
    "\n",
    "opt = Opt(:LD_MMA, 2)\n",
    "lower_bounds!(opt, [-Inf, 0.])\n",
    "xtol_rel!(opt,1e-4)\n",
    "\n",
    "min_objective!(opt, myfunc)\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x,g,2,0), 1e-8)\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x,g,-1,1), 1e-8)\n",
    "\n",
    "(minfunc,minx,ret) = NLopt.optimize(opt, [1.234, 5.678])\n",
    "println(\"got $minfunc at $minx after $count iterations returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NLopt: Rosenbrock\n",
    "\n",
    "* Let's tackle the rosenbrock example again.\n",
    "* To make it more interesting, let's add an inequality constraint.\n",
    "\t$$ \\min_{x\\in \\mathbb{R}^2} (1-x_1)^2  + 100(x_2-x_1^2)^2  \\text{  subject to  } 0.8 - x_1^2 -x_2^2 \\geq 0 $$\n",
    "* in `NLopt` format, the constraint is $x_1^2 + x_2^2 - 0.8 \\leq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function rosenbrockf(x::Vector,grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "        grad[2] = 200.0 * (x[2] - x[1]^2)\n",
    "    end\n",
    "    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function r_constraint(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "\tgrad[1] = 2*x[1]\n",
    "\tgrad[2] = 2*x[2]\n",
    "\tend\n",
    "\treturn x[1]^2 + x[2]^2 - 0.8\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grad = zeros(2)\n",
    "xrange = collect(-2.5:0.01:2)\n",
    "cc = contour(xrange,xrange, (x,y)->sqrt(rosenbrockf([x, y],grad)), fill=true, color=:deep, ylab = L\"x_2\",xlab = L\"x_1\", leg = :bottomleft)\n",
    "contour!(cc,xrange,xrange,(x,y)->r_constraint([x, y],grad), levels = [0])\n",
    "scatter!(cc,[1],[1],color = :red, lab = \"unconstrained optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# let's compute the constrained optimizer now!\n",
    "opt = Opt(:LD_MMA, 2)\n",
    "lower_bounds!(opt, [-5, -5.0])\n",
    "min_objective!(opt,(x,g) -> rosenbrockf(x,g))\n",
    "inequality_constraint!(opt, (x,g) -> r_constraint(x,g))\n",
    "ftol_rel!(opt,1e-9)\n",
    "(minfunc,minx,ret) = NLopt.optimize(opt, [-1.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scatter!(cc, [minx[1]], [minx[2]], label = \"constrained optimizer\", leg = :topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JuMP.jl\n",
    "\n",
    "* Introduce [`JuMP.jl`](https://jump.dev/)\n",
    "* JuMP is a mathematical programming interface for Julia. It is like AMPL, but for free and with a decent programming language.\n",
    "* The main highlights are:\n",
    "\t* It uses automatic differentiation to compute derivatives from your expression.\n",
    "\t* It supplies this information, as well as the sparsity structure of the Hessian to your preferred solver.\n",
    "\t* It decouples your problem completely from the type of solver you are using. This is great, since you don't have to worry about different solvers having different interfaces.\n",
    "\t* In order to achieve this, `JuMP` uses [`MathProgBase.jl`](https://github.com/JuliaOpt/MathProgBase.jl), which converts your problem formulation into a standard representation of an optimization problem.\n",
    "* Let's look at the readme\n",
    "* The technical citation is Lubin et al <cite data-cite=JuMP></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## JuMP: Quick start guide\n",
    "\n",
    "* this is form the [quick start guide](https://jump.dev/JuMP.jl/stable/quickstart/#Quick-Start-Guide)\n",
    "* please check the docs, they are excellent.\n",
    "\n",
    "### Step 1: create a model\n",
    "\n",
    "* A model collects variables, objective function and constraints.\n",
    "* it defines a specific solver to be used.\n",
    "* JuMP makes it very easy to [swap out solver backends](http://www.juliaopt.org/JuMP.jl/dev/installation/) - This is very valuable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using GLPK\n",
    "model = Model(with_optimizer(GLPK.Optimizer))\n",
    "@variable(model, 0 <= x <= 2)\n",
    "@variable(model, 0 <= y <= 30)\n",
    "# next, we set an objective function\n",
    "@objective(model, Max, 5x + 3 * y)\n",
    "\n",
    "# maybe add a constraint called \"con\"?\n",
    "@constraint(model, con, 1x + 5y <= 3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* At this stage `JuMP` has a mathematical representation of our model internalized\n",
    "* The `MathProgBase` machinery knows now exactly how to translate that to different solver interfaces\n",
    "* For us the only thing left: hit the button!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "JuMP.optimize!(model)\n",
    "\n",
    "# look at status\n",
    "termination_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we query objective value and solutions\n",
    "@show objective_value(model)\n",
    "@show value(x)\n",
    "@show value(y)\n",
    "\n",
    "# as well as the value of the dual variable on the constraint\n",
    "@show dual(con);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The last call gets the *dual value associated with a constraint*\n",
    "* Economists most of the time call that the *value of the lagrange multiplier*. \n",
    "\n",
    "> For linear programs, a feasible dual on a `>=` constraint is nonnegative and a feasible dual on a `<=` constraint is nonpositive\n",
    "\n",
    "* This is different to some textbooks and has nothing to do with wether max or minimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# helpfully, we have this, which is always positive:\n",
    "shadow_price(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## JuMP handles...\n",
    "\n",
    "* linear programming\n",
    "* mixed-integer programming\n",
    "* second-order conic programming\n",
    "* semidefinite programming, and \n",
    "* nonlinear programming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# JuMP: nonlinear Rosenbrock Example\n",
    "# Instead of hand-coding first and second derivatives, you only have to give `JuMP` expressions for objective and constraints.\n",
    "# Here is an example.\n",
    "\n",
    "using Ipopt\n",
    "\n",
    "let\n",
    "\n",
    "    m = Model(with_optimizer(Ipopt.Optimizer))\n",
    "\n",
    "    @variable(m, x)\n",
    "    @variable(m, y)\n",
    "\n",
    "    @NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "    JuMP.optimize!(m)\n",
    "    @show value(x)\n",
    "    @show value(y)\n",
    "    @show termination_status(m)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# not bad, right?\n",
    "# adding the constraint from before:\n",
    "\n",
    "let\n",
    "    \n",
    "    m = Model(with_optimizer(Ipopt.Optimizer))\n",
    "\n",
    "    @variable(m, x)\n",
    "    @variable(m, y)\n",
    "\n",
    "    @NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "\n",
    "    @NLconstraint(m,x^2 + y^2 <= 0.8)\n",
    "\n",
    "    JuMP.optimize!(m)\n",
    "    @show value(x)\n",
    "    @show value(y)\n",
    "    @show termination_status(m)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## JuMP: Maximium Likelihood\n",
    "\n",
    "* Let's redo the maximum likelihood example in JuMP.\n",
    "* Let $\\mu,\\sigma^2$ be the unknown mean and variance of a random sample generated from the normal distribution.\n",
    "* Find the maximum likelihood estimator for those parameters!\n",
    "* density:\n",
    "\n",
    "$$ f(x_i|\\mu,\\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Likelihood Function\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "L(\\mu,\\sigma^2) = \\Pi_{i=1}^N f(x_i|\\mu,\\sigma^2) =& \\frac{1}{(\\sigma \\sqrt{2\\pi})^n} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 \\right) \\\\\n",
    "\t =& \\left(\\sigma^2 2\\pi\\right)^{-\\frac{n}{2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 \\right) \n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "* Constraints: $\\mu\\in \\mathbb{R},\\sigma>0$\n",
    "* log-likelihood: \n",
    "\n",
    "$$ \\log L = l = -\\frac{n}{2} \\log \\left( 2\\pi \\sigma^2 \\right) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#  Copyright 2015, Iain Dunning, Joey Huchette, Miles Lubin, and contributors\n",
    "#  example modified \n",
    "using Distributions\n",
    "\n",
    "let\n",
    "    distrib = Normal(4.5,3.5)\n",
    "    n = 10000\n",
    "    \n",
    "    data = rand(distrib,n);\n",
    "    \n",
    "    m = Model(with_optimizer(Ipopt.Optimizer))\n",
    "    set_optimizer_attribute(m, MOI.Silent(), true)\n",
    "\n",
    "    @variable(m, mu, start = 0.0)\n",
    "    @variable(m, sigma >= 0.0, start = 1.0)\n",
    "    \n",
    "    @NLobjective(m, Max, -(n/2)*log(2π*sigma^2)-sum((data[i] - mu) ^ 2 for i = 1:n)/(2*sigma^2))\n",
    "    \n",
    "    JuMP.optimize!(m)\n",
    "    @show termination_status(m)\n",
    "    println(\"μ = \", value(mu),\", mean(data) = \", mean(data))\n",
    "    println(\"σ^2 = \", value(sigma)^2, \", var(data) = \", var(data))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Constrained Problems (LPs)\n",
    "\n",
    "* Very similar to before, just that both objective and constraints are *linear*.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x} & \\mathbf{c}^T \\mathbf{x}\\\\\n",
    "\\text{subject to } & \\mathbf{w}_{LE}^{(i)T} \\mathbf{x} \\leq b_i \\text{ for  }i\\in{1,2,3,\\dots}\\\\\n",
    "& \\mathbf{w}_{GE}^{(j)T} \\mathbf{x} \\geq b_j \\text{ for  }j\\in{1,2,3,\\dots}\\\\\n",
    " & \\mathbf{w}_{EQ}^{(k)T} \\mathbf{x} = b_k \\text{ for  }k\\in{1,2,3,\\dots}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Our initial JuMP example was of that sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standard Form\n",
    "\n",
    "* Usually LPs are given in *standard form*\n",
    "* All constraints are less-than inequalities\n",
    "* All choice variables are non-negative.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x}    & \\mathbf{c}^T \\mathbf{x}\\\\\n",
    "\\text{subject to } & \\mathbf{A}\\mathbf{x} \\leq b\\\\\n",
    "                   & \\mathbf{x}\\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Greater-than inequality constraints are inverted\n",
    "* equality constraints are split into two\n",
    "* $\\mathbf{x} = \\mathbf{x}^+ - \\mathbf{x}^-$ and we constrain both components to be positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Equality Form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x}    & \\mathbf{c}^T \\mathbf{x}\\\\\n",
    "\\text{subject to } & \\mathbf{A}\\mathbf{x} = b\\\\\n",
    "                   & \\mathbf{x}\\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Can transform standard into equality form \n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x} \\leq b \\to \\mathbf{A}\\mathbf{x} + \\mathbf{s}= b ,\\mathbf{s}\\geq 0\n",
    "$$\n",
    "\n",
    "* equality constraints are split into two\n",
    "* $\\mathbf{x} = \\mathbf{x}^+ - \\mathbf{x}^-$ and we constrain both components to be positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving LPs\n",
    "\n",
    "* Simplex Algorithm operates on Equality Form\n",
    "* Moving from one vertex to the next of the feasible set, this is guaranteed to find the optimal solution if the problem is bounded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Cannery Problem\n",
    "\n",
    "* A can factory (a cannery) has plants in Seattle and San Diego\n",
    "* They need to decide how to serve markets New-York, Chicago, Topeka\n",
    "* Firm wants to \n",
    "    1. minimize shipping costs\n",
    "    2. shipments cannot exceed capacity\n",
    "    3. shipments must satisfy demand\n",
    "* Formalize that!\n",
    "* Plant capacity $cap_i$, demands $d_j$ and transport costs from plant $i$ to market  $j$, $dist_{i,j} c$ are all given.\n",
    "* Let $\\mathbf{x}$ be a matrix with element $x_{i,j}$ for number of cans shipped from $i$ to $j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From Maths ...\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x}    & \\sum_{i=1}^2 \\sum_{j=1}^3 dist_{i,j}c \\times x_{i,j}\\\\\n",
    "\\text{subject to } & \\sum_{j=1}^3 x(i,j) \\leq cap_i , \\forall i \\\\\n",
    "                   & \\sum_{i=1}^2 x(i,j) \\geq d_j , \\forall j \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ... to JuMP\n",
    "# https://github.com/JuliaOpt/JuMP.jl/blob/release-0.19/examples/cannery.jl\n",
    "#  Copyright 2017, Iain Dunning, Joey Huchette, Miles Lubin, and contributors\n",
    "#  This Source Code Form is subject to the terms of the Mozilla Public\n",
    "#  License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "#  file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "#############################################################################\n",
    "# JuMP\n",
    "# An algebraic modeling language for Julia\n",
    "# See http://github.com/JuliaOpt/JuMP.jl\n",
    "#############################################################################\n",
    "\n",
    "using JuMP, GLPK, Test\n",
    "const MOI = JuMP.MathOptInterface\n",
    "\n",
    "\"\"\"\n",
    "    example_cannery(; verbose = true)\n",
    "JuMP implementation of the cannery problem from Dantzig, Linear Programming and\n",
    "Extensions, Princeton University Press, Princeton, NJ, 1963.\n",
    "Author: Louis Luangkesorn\n",
    "Date: January 30, 2015\n",
    "\"\"\"\n",
    "function example_cannery(; verbose = true)\n",
    "    plants = [\"Seattle\", \"San-Diego\"]\n",
    "    markets = [\"New-York\", \"Chicago\", \"Topeka\"]\n",
    "\n",
    "    # Capacity and demand in cases.\n",
    "    capacity = [350, 600]\n",
    "    demand = [300, 300, 300]\n",
    "\n",
    "    # Distance in thousand miles.\n",
    "    distance = [2.5 1.7 1.8; 2.5 1.8 1.4]\n",
    "\n",
    "    # Cost per case per thousand miles.\n",
    "    freight = 90\n",
    "\n",
    "    num_plants = length(plants)\n",
    "    num_markets = length(markets)\n",
    "\n",
    "    cannery = Model(with_optimizer(GLPK.Optimizer))\n",
    "\n",
    "    @variable(cannery, ship[1:num_plants, 1:num_markets] >= 0)\n",
    "\n",
    "    # Ship no more than plant capacity\n",
    "    @constraint(cannery, capacity_con[i in 1:num_plants],\n",
    "        sum(ship[i,j] for j in 1:num_markets) <= capacity[i]\n",
    "    )\n",
    "\n",
    "    # Ship at least market demand\n",
    "    @constraint(cannery, demand_con[j in 1:num_markets],\n",
    "        sum(ship[i,j] for i in 1:num_plants) >= demand[j]\n",
    "    )\n",
    "\n",
    "    # Minimize transporatation cost\n",
    "    @objective(cannery, Min, sum(distance[i, j] * freight * ship[i, j]\n",
    "        for i in 1:num_plants, j in 1:num_markets)\n",
    "    )\n",
    "\n",
    "    JuMP.optimize!(cannery)\n",
    "\n",
    "    if verbose\n",
    "        println(\"RESULTS:\")\n",
    "        for i in 1:num_plants\n",
    "            for j in 1:num_markets\n",
    "                println(\"  $(plants[i]) $(markets[j]) = $(JuMP.value(ship[i, j]))\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @test JuMP.termination_status(cannery) == MOI.OPTIMAL\n",
    "    @test JuMP.primal_status(cannery) == MOI.FEASIBLE_POINT\n",
    "    @test JuMP.objective_value(cannery) == 151200.0\n",
    "end\n",
    "example_cannery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discrete Optimization / Integer Programming\n",
    "\n",
    "* Here the choice variable is contrained to come from a discrete set $\\mathcal{X}$. \n",
    "* If this set is $\\mathcal{X} = \\mathbb{N}$, we have an **integer program**\n",
    "* If only *some* $x$ have to be discrete, this is a **mixed integer program**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_\\mathbf{x}    & x_1 + x_2\\\\\n",
    "\\text{subject to } & ||\\mathbf{x}|| \\leq 2\\\\\n",
    "                   & \\mathbf{x} \\in \\mathbb{N}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* continuous optimum is $(-\\sqrt{2},-\\sqrt{2})$ and objective is $y=-2\\sqrt{2}=-2.828$\n",
    "* Integer constrained problem is only delivering $y=-2$, and $\\mathbf{x}^*\\in {(-2,0),(-1,-1),(0,-2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = -3:0.01:3\n",
    "dx = repeat(range(-3,stop = 3, length = 7),1,7)\n",
    "contourf(x,x,(x,y)->x+y,color=:blues)\n",
    "scatter!(dx,dx',legend=false,markercolor=:white)\n",
    "plot!(x->sqrt(4-x^2),-2,2,c=:white)\n",
    "plot!(x->-sqrt(4-x^2),-2,2,c=:white)\n",
    "scatter!([-2,-1,0],[0,-1,-2],c=:red)\n",
    "scatter!([-sqrt(2)],[-sqrt(2)],c=:red,markershape=:cross,markersize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rounding\n",
    "\n",
    "* One solution is to just *round the continuous solution to the nearest integer*\n",
    "* We compute the **relaxed** problem, i.e. the one where $x$ is continuous.\n",
    "* Then we round up or down.\n",
    "* Can go terribly wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cutting Planes\n",
    "\n",
    "* This is an exact method\n",
    "* We solve the relaxed problem first.\n",
    "* Then we add linear constraints that result in the solution becoming integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Branch and Bound\n",
    "\n",
    "* This enumerates all possible soultions.\n",
    "* Branch and bound does this, without having to compute all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: The Knapsack Problem\n",
    "\n",
    "* We are packing our knapsack for a trip but only have space for the most valuable items.\n",
    "* We have $x_i=0$ if item $i$ is not in the sack, 1 else.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x & - \\sum_{i=1}^n v_i x_i \\\\\n",
    "\\text{s.t. } & \\sum_{i=1}^n w_i x_i \\leq w_{max} \\\\\n",
    "w_i \\in \\mathbb{N}_+,  & v_i \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* If ther are $n$ items, we have $2^n$ possible design vectors.\n",
    "* But there is a useful recursive relationship.\n",
    "* If we solved $n-1$ knapsack problems so far and deliberate about item $n$\n",
    "    * If it's not worth including item $n$, then the solution **is** the knapsack problem for $n-1$ items and capacity $w_{\\max}$\n",
    "    * If it IS worth including it: solution will have value of knapsack with $n-1$ items and reduced capacity, plus the value of the new item\n",
    "* This **is** dynamic progamming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Knacksack Recursion\n",
    "\n",
    "* In particular, the recursion looks like this:\n",
    "\n",
    "$$\n",
    "\\text{knapsack}\\left(i,w_{\\text{max}}\\right)=\\begin{cases}\n",
    "0 & \\text{if}i=0\\\\\n",
    "\\text{knapsack}\\left(i-1,w_{\\text{max}}\\right) & \\text{if}w_{i}>w_{\\text{max}}\\\\\n",
    "\\max\\begin{cases}\n",
    "\\text{knapsack}\\left(i-1,w_{\\text{max}}\\right) & \\text{(discard new item)}\\\\\n",
    "\\text{knapsack}\\left(i-1,w_{\\text{max}}-w_{i}\\right)+v_{i} & \\text{(include new item)}\n",
    "\\end{cases} & \\text{else.}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#  Copyright 2017, Iain Dunning, Joey Huchette, Miles Lubin, and contributors\n",
    "#  This Source Code Form is subject to the terms of the Mozilla Public\n",
    "#  License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "#  file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "#############################################################################\n",
    "# JuMP\n",
    "# An algebraic modeling langauge for Julia\n",
    "# See http://github.com/JuliaOpt/JuMP.jl\n",
    "#############################################################################\n",
    "# knapsack.jl\n",
    "#\n",
    "# Solves a simple knapsack problem:\n",
    "# max sum(p_j x_j)\n",
    "#  st sum(w_j x_j) <= C\n",
    "#     x binary\n",
    "#############################################################################\n",
    "\n",
    "using JuMP, Cbc, LinearAlgebra\n",
    "\n",
    "let\n",
    "\n",
    "    # Maximization problem\n",
    "    m = Model(with_optimizer(Cbc.Optimizer))\n",
    "    set_optimizer_attribute(m, MOI.Silent(), true)\n",
    "\n",
    "    \n",
    "    @variable(m, x[1:5], Bin)\n",
    "    \n",
    "    profit = [ 5, 3, 2, 7, 4 ]\n",
    "    weight = [ 2, 8, 4, 2, 5 ]\n",
    "    capacity = 10\n",
    "    \n",
    "    # Objective: maximize profit\n",
    "    @objective(m, Max, dot(profit, x))\n",
    "    \n",
    "    # Constraint: can carry all\n",
    "    @constraint(m, dot(weight, x) <= capacity)\n",
    "    \n",
    "    # Solve problem using MIP solver\n",
    "    JuMP.optimize!(m)\n",
    "    \n",
    "    println(\"Objective is: \", JuMP.objective_value(m))\n",
    "    println(\"Solution is:\")\n",
    "    for i = 1:5\n",
    "        print(\"x[$i] = \", JuMP.value(x[i]))\n",
    "        println(\", p[$i]/w[$i] = \", profit[i]/weight[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "f9fc27fbe6a14ca1a30696befe9c1b3e",
   "lastKernelId": "82bbbff1-cee9-40f2-ae88-402e94a79901"
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
